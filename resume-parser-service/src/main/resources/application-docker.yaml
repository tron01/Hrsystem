spring:
  ai:
    ollama:
      base-url: http://ollama:11434
      chat:
        model: resume-sf # your custom model created from Modelfile
        options:
          temperature: 0
          keep-alive: 5m
  kafka:
    bootstrap-servers: kafka1:9092
    consumer:
      group-id: resume-parser-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      auto-offset-reset: earliest
      properties:
        spring.json.value.default.type: com.Abhijith.resume_parser_service.kafka.event.ResumeParseEvent
        spring.json.trusted.packages: "com.Abhijith.resume_parser_service.kafka.event"
        spring.json.use.type.headers: false
    listener:
      ack-mode: manual
  threads:
    virtual:
      enabled: true
  data:
    mongodb:
      uri: mongodb://root:root@mongodb:27017/applicationdb?authSource=admin

server:
  port: 0 # Use random port for Docker

eureka:
  client:
    service-url:
      defaultZone: http://eureka-server:8761/eureka/
    register-with-eureka: true
    fetch-registry: true
    instance-info-replication-interval-seconds: 10
  instance:
    prefer-ip-address: true
    instance-id: ${spring.application.name}:${random.value}
    metadata-map:
      instanceId: ${spring.application.name}:${random.value}

kafka:
  topics:
    resume-parse: resume-parse-topic

management:
  tracing:
    sampling:
      probability: 1.0
  endpoints:
    web:
      exposure:
        include: "*"

logging:
  level:
    com.Abhijith.resume_parser_service: DEBUG
  pattern:
    level: "%5p [traceId=%X{traceId} spanId=%X{spanId}] %m%n"
